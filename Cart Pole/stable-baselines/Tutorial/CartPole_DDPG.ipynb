{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose >= 1:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose >= 1:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1, file_name: str = None):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"Best Models\", file_name)\n",
    "        self.best_reward = -np.inf\n",
    "        self.prev_mean_reward = 0.0\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), \"episodes\")\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 10 episodes\n",
    "              last_ep = x[-1]\n",
    "              last_reward = y[-1] \n",
    "              mean_reward = np.mean(y[-10:])\n",
    "              delta_mearn_reward = mean_reward - self.prev_mean_reward\n",
    "              self.prev_mean_reward = mean_reward\n",
    "              if self.verbose >= 1:\n",
    "                print(f\"Num timesteps: {self.num_timesteps} - Ep: {last_ep}\")\n",
    "                print(f\"Best reward: {self.best_reward:.2f} - Last reward per episode: {last_reward:.2f} - Mean reward last 10 episode: {mean_reward:.2f} - Delta mean: {delta_mearn_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if last_reward > self.best_reward:\n",
    "                  self.best_reward = last_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose >= 1:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610] [1.99576400e+00 3.02207500e+00 2.94331500e+00 1.73427310e+01\n",
      " 2.18816380e+01 6.98597700e-01 2.08614400e+01 6.72972640e+01\n",
      " 8.14585000e+00 9.66341500e+00 1.45813290e+00 5.00899620e+01\n",
      " 1.24633540e+01 8.97480700e+00 5.76419930e+00 7.53295900e+00\n",
      " 6.25493100e+00 2.35060260e+00 1.88172050e+01 1.65341280e+00\n",
      " 3.20337300e+01 6.47229200e+00 3.33778300e+01 4.94356040e+01\n",
      " 1.62819730e+00 4.82549570e+00 5.43000000e-01 1.61446860e+01\n",
      " 2.81682470e+00 1.48163370e+00 7.36359060e-01 3.78804100e+01\n",
      " 2.37044620e+00 2.54740600e+00 3.09483740e+00 3.94515090e+00\n",
      " 2.83455960e+01 1.05697820e+00 1.36650150e+00 2.10091040e+00\n",
      " 1.05319310e+01 9.87352900e-01 2.91943260e+01 9.92872200e+00\n",
      " 1.77578030e+00 2.50345640e+00 6.23408900e+00 2.91669130e+00\n",
      " 2.06937480e-01 2.97383270e+01 6.72158960e-01 9.60555100e-01\n",
      " 2.15330960e+00 1.10019800e+00 1.03655220e+00 5.09833150e+00\n",
      " 6.73513400e-01 8.21822700e+00 1.61355410e+00 3.12754700e+00\n",
      " 5.41652770e+00 2.31107100e+00 1.98826580e+01 4.41231960e+01\n",
      " 2.02793550e+00 1.88807560e+00 2.50507890e+01 2.17692200e+00\n",
      " 2.29441480e+00 2.13955120e+00 1.48147790e+00 2.07825200e+00\n",
      " 3.48407670e+00 5.98890200e+00 3.03992330e+01 2.96134920e+00\n",
      " 3.82813000e+01 9.25605500e+00 1.82123860e+00 3.91668030e+00\n",
      " 3.06400800e+00 6.69743300e+00 4.98107200e+00 6.43720200e+00\n",
      " 1.92239950e+00 3.62253950e+00 2.80601500e+00 2.53224560e+00\n",
      " 3.62900770e+01 3.26183630e+00 2.12969780e+00 1.75994810e+00\n",
      " 3.46403100e+00 2.13946100e+00 4.77045020e+01 4.60102940e+00\n",
      " 2.34820200e+00 5.54627850e+00 3.09788630e+00 2.88161470e+00\n",
      " 3.82458230e+01 2.58665050e+01 6.86725950e+00 3.79047000e+00\n",
      " 3.27170830e+01 1.06124504e+02 2.16402600e+00 4.03275260e+01\n",
      " 2.37942980e+00 2.18321820e+00 2.89903140e+00 5.56558300e+00\n",
      " 3.32237240e+00 2.67855760e+00 7.62182160e+01 1.93754600e+01\n",
      " 1.48465240e+00 6.95885200e+01 2.43365120e+00 1.37547410e+00\n",
      " 2.98330970e+00 2.95580270e+01 2.03274610e+02 2.59192280e+00\n",
      " 3.04443030e+00 2.79430580e+00 2.00867770e+00 3.13995080e+00\n",
      " 3.09828110e+00 1.02916950e+02 1.62437420e+01 2.11213370e+00\n",
      " 1.57003950e+00 2.78067260e+00 2.26284890e+00 4.86064340e+00\n",
      " 2.43498540e+00 9.73467900e+01 1.35172370e+00 1.18870544e+02\n",
      " 5.60039470e+00 1.32670250e+01 3.06816340e+00 5.77552070e+01\n",
      " 3.34424970e+00 2.68760800e+01 2.15832140e+02 9.58287200e-01\n",
      " 3.05271500e+00 9.93271940e+01 7.01488950e+01 1.66525860e+02\n",
      " 3.13491250e+01 9.51895000e+01 8.77263300e+01 3.23469300e+00\n",
      " 2.33928420e+02 3.02261830e+00 3.29126900e+01 1.51762600e+02\n",
      " 2.24497400e+01 3.08419400e+00 4.25487860e+01 2.80271360e+00\n",
      " 1.53702080e+01 3.99042560e+00 1.99639100e+02 2.00328810e+02\n",
      " 2.02534800e+02 2.61147640e+02 1.96958600e+02 2.12482150e+02\n",
      " 9.67844400e+01 1.58494110e+02 8.70809250e+01 1.89627290e+02\n",
      " 1.14174860e+02 1.73007610e+02 2.10968140e+02 2.28031950e+02\n",
      " 1.32209110e+01 2.16813550e+02 2.54106340e+02 2.26933620e+02\n",
      " 2.20448600e+02 1.95516780e+02 1.73802920e+02 7.63748860e+01\n",
      " 2.16798870e+02 2.33353030e+02 2.40844480e+02 2.51619550e+02\n",
      " 2.55593570e+02 2.49072970e+02 2.18079820e+02 2.42769180e+02\n",
      " 2.67140200e+02 2.37916800e+02 2.60256400e+02 2.58715760e+02\n",
      " 2.26862210e+02 2.52796800e+02 2.56326230e+02 2.69719060e+02\n",
      " 2.75073820e+02 2.44871290e+02 2.36225450e+02 2.58464870e+02\n",
      " 2.45612640e+02 2.46865480e+02 2.89753940e+02 2.69007780e+02\n",
      " 3.10680180e+02 2.67916230e+02 2.51360080e+02 2.59949980e+02\n",
      " 2.41874540e+02 2.42978560e+02 2.66783450e+02 2.62563800e+02\n",
      " 2.69644130e+02 2.52627600e+02 2.65822900e+02 2.74545470e+02\n",
      " 2.55959380e+02 2.44143810e+02 2.63013980e+02 2.56472700e+02\n",
      " 2.92805540e+02 2.78900200e+02 2.56792500e+02 2.68138700e+02\n",
      " 2.62315000e+02 2.73852780e+02 2.73826400e+02 2.77679300e+02\n",
      " 2.67643770e+02 2.55205520e+02 3.05123700e+02 2.75622040e+02\n",
      " 2.84736900e+02 2.91718000e+02 2.69268860e+02 2.73082460e+02\n",
      " 2.77389000e+02 3.02880300e+02 2.76640900e+02 2.76148250e+02\n",
      " 2.85107180e+02 2.82679800e+02 2.93624600e+02 2.71680660e+02\n",
      " 2.72948500e+02 2.89685060e+02 2.70777680e+02 2.94250240e+02\n",
      " 2.75336820e+02 3.21228000e+02 2.81186550e+02 2.79134280e+02\n",
      " 2.79751160e+02 2.93815460e+02 2.93851650e+02 2.97597260e+02\n",
      " 2.83814850e+02 2.71893460e+02 2.95876000e+02 2.89290100e+02\n",
      " 2.93341460e+02 3.10327600e+02 3.20792400e+02 3.01193300e+02\n",
      " 2.88619600e+02 3.03788330e+02 2.88760620e+02 3.01823150e+02\n",
      " 2.85339500e+02 3.12446140e+02 3.16162840e+02 3.17893200e+02\n",
      " 2.98368700e+02 3.03729280e+02 3.19276370e+02 3.12324980e+02\n",
      " 2.95836200e+02 3.00891750e+02 3.17575350e+02 3.15112030e+02\n",
      " 3.03274630e+02 3.04900020e+02 2.98214660e+02 3.12038180e+02\n",
      " 2.87828800e+02 3.06272860e+02 2.78816600e+02 2.98242830e+02\n",
      " 3.14630800e+02 3.12262100e+02 2.84905400e+02 3.19109740e+02\n",
      " 3.03624600e+02 2.94884030e+02 3.12828460e+02 3.33573060e+02\n",
      " 2.94411400e+02 3.29600430e+02 3.14529940e+02 3.25833830e+02\n",
      " 3.15557430e+02 3.00767500e+02 3.02917940e+02 2.86873300e+02\n",
      " 3.13836800e+02 3.12447880e+02 3.03549500e+02 2.92792940e+02\n",
      " 3.13931400e+02 3.11730300e+02 3.13358300e+02 3.02605200e+02\n",
      " 3.10452120e+02 3.08560240e+02 3.03838930e+02 3.13591100e+02\n",
      " 3.41303100e+02 2.94321800e+02 3.18060500e+02 3.13828600e+02\n",
      " 3.18315100e+02 3.20932340e+02 3.09333340e+02 3.18367650e+02\n",
      " 3.22565670e+02 3.24467770e+02 3.28170720e+02 3.22369750e+02\n",
      " 3.39796600e+02 3.04672500e+02 3.25800350e+02 3.66033400e+02\n",
      " 3.20271820e+02 3.03581450e+02 3.20520400e+02 3.19034670e+02\n",
      " 3.04422550e+02 3.16092130e+02 3.29675960e+02 3.31038850e+02\n",
      " 3.38231500e+02 3.43386470e+02 3.43453740e+02 3.20692260e+02\n",
      " 3.39182250e+02 3.27294950e+02 3.22113770e+02 3.21767640e+02\n",
      " 3.43025970e+02 3.43418030e+02 3.15186130e+02 3.37688260e+02\n",
      " 3.45758000e+02 3.45718900e+02 3.39842650e+02 3.59007900e+02\n",
      " 3.65857270e+02 3.28846650e+02 3.31678440e+02 3.19981450e+02\n",
      " 3.37986200e+02 3.39407650e+02 3.42398740e+02 3.46820530e+02\n",
      " 3.38237640e+02 3.45878170e+02 3.45064450e+02 3.65979700e+02\n",
      " 3.18513120e+02 3.27664370e+02 3.42601320e+02 3.46171840e+02\n",
      " 3.42496670e+02 3.41524800e+02 3.49494540e+02 3.50188450e+02\n",
      " 3.33280940e+02 3.59039340e+02 3.66994300e+02 3.48211000e+02\n",
      " 3.42651460e+02 3.58722380e+02 3.67862060e+02 3.67479770e+02\n",
      " 3.42806150e+02 3.31261350e+02 3.43668880e+02 3.41946620e+02\n",
      " 3.47950300e+02 3.36510600e+02 3.42089780e+02 3.41070530e+02\n",
      " 3.56044950e+02 3.48357900e+02 3.61667360e+02 3.45442840e+02\n",
      " 3.45097530e+02 3.43733060e+02 3.59398680e+02 3.66429900e+02\n",
      " 3.43429170e+02 3.57222800e+02 3.40274900e+02 3.50281400e+02\n",
      " 3.70733860e+02 3.43499600e+02 3.40464020e+02 3.69779500e+02\n",
      " 3.45546420e+02 3.53162400e+02 3.67041440e+02 3.41356050e+02\n",
      " 3.69663240e+02 3.45764600e+02 3.67105350e+02 3.44086030e+02\n",
      " 3.56636960e+02 3.60174350e+02 3.42713040e+02 3.48282800e+02\n",
      " 3.60809630e+02 3.68638430e+02 3.46230560e+02 3.41990450e+02\n",
      " 3.40899480e+02 3.48064730e+02 3.37480070e+02 3.67967470e+02\n",
      " 3.67750900e+02 3.68452820e+02 3.67220580e+02 3.43555800e+02\n",
      " 3.45203120e+02 3.49479500e+02 3.67159300e+02 3.60892270e+02\n",
      " 3.46589970e+02 3.45808900e+02 3.65007420e+02 3.45760680e+02\n",
      " 3.67314880e+02 3.65084470e+02 3.66744420e+02 3.65448970e+02\n",
      " 3.46893250e+02 3.60855800e+02 3.66655820e+02 3.49132660e+02\n",
      " 3.49296540e+02 3.70264100e+02 3.61237430e+02 3.65828700e+02\n",
      " 3.71388150e+02 3.63231600e+02 3.59847260e+02 3.62449980e+02\n",
      " 3.62956570e+02 3.71270100e+02 3.68094730e+02 3.64710330e+02\n",
      " 3.68136930e+02 3.69335420e+02 3.63517200e+02 3.86070500e+02\n",
      " 3.41790340e+02 3.68578250e+02 3.83269700e+02 3.68643160e+02\n",
      " 4.16030800e+02 3.71529900e+02 3.71005370e+02 3.74318480e+02\n",
      " 3.87072600e+02 3.63275760e+02 3.65621900e+02 3.71712070e+02\n",
      " 3.47619350e+02 3.60771480e+02 3.61526100e+02 3.52475200e+02\n",
      " 3.60310520e+02 3.70598400e+02 3.68966640e+02 3.78064760e+02\n",
      " 3.86320220e+02 3.59653400e+02 3.63486420e+02 3.64994000e+02\n",
      " 3.65376920e+02 3.63257800e+02 3.60037630e+02 3.62169980e+02\n",
      " 3.71407840e+02 3.65305730e+02 3.60213800e+02 3.59826780e+02\n",
      " 3.65045720e+02 3.70335330e+02 3.58791900e+02 3.94881400e+02\n",
      " 3.84405500e+02 3.59968870e+02 3.68768340e+02 3.91400270e+02\n",
      " 3.98146550e+02 3.92817600e+02 3.68122530e+02 3.86412140e+02\n",
      " 3.71510220e+02 4.04405550e+02 3.95897500e+02 3.70002800e+02\n",
      " 3.89511540e+02 3.69150100e+02 3.92236020e+02 3.92705870e+02\n",
      " 3.68879430e+02 3.67577600e+02 3.77895050e+02 3.92088600e+02\n",
      " 3.91695700e+02 3.96904660e+02 3.96587430e+02 3.66786680e+02\n",
      " 3.93855350e+02 3.70605650e+02 3.67629600e+02 3.90381680e+02\n",
      " 3.65040530e+02 3.92732760e+02 3.67401180e+02 3.90739350e+02\n",
      " 3.61831420e+02 3.53807100e+02 3.68265050e+02 3.93831730e+02\n",
      " 3.92965670e+02 3.69320920e+02 3.68121800e+02 3.84603030e+02\n",
      " 3.81666200e+02 4.05211100e+02 3.69802220e+02 3.64413670e+02\n",
      " 3.86697570e+02 4.05848480e+02 3.93968780e+02 3.90811860e+02\n",
      " 3.65107030e+02 3.66429960e+02 3.68341920e+02 3.80805480e+02\n",
      " 3.65146480e+02 3.67215120e+02 3.63804400e+02 3.90197570e+02\n",
      " 4.15844150e+02 3.93427760e+02 3.89302120e+02 3.69102170e+02\n",
      " 3.94681670e+02 3.87742340e+02 3.79820680e+02 3.89700500e+02\n",
      " 3.84454830e+02 3.92434020e+02 3.92821320e+02 3.68947660e+02\n",
      " 3.92597630e+02 3.86500850e+02 3.88647460e+02 3.89228580e+02\n",
      " 3.90328200e+02 3.82024200e+02 3.87890780e+02 3.92581670e+02\n",
      " 4.02897030e+02 3.89198330e+02 3.88336640e+02 4.10088870e+02\n",
      " 3.92614260e+02 3.91248500e+02 3.86560060e+02 4.23108700e+02\n",
      " 3.92161700e+02 3.91257540e+02 3.91953670e+02 3.90683000e+02\n",
      " 3.92904800e+02 3.90128100e+02 3.83173980e+02 4.17342200e+02\n",
      " 4.17153350e+02 4.16924470e+02 4.13350280e+02 4.12676420e+02\n",
      " 3.87423400e+02 4.17448180e+02 4.08737430e+02 4.16510440e+02\n",
      " 4.15501600e+02 4.18144380e+02 3.89998960e+02]\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"tmpa/\"\n",
    "x, y = ts2xy(load_results(log_dir), \"episodes\")\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 106  311  490  923 1341 1432 1932 2432] [ 1.995764   3.022075   2.943315  17.342731  21.881638   0.6985977\n",
      " 20.86144   67.297264 ]\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"tmpa/\"\n",
    "x, y = ts2xy(load_results(log_dir), \"timesteps\")\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2143b53ca04521b7cd39ed3fe1e4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 1000 - Ep: 23\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 1000 - Ep: 23\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: -inf - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 3.97\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: -inf - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 3.97\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saving new best model to Training\\Save Models\\Best Models\\DDPG_ip_1_100000\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saving new best model to Training\\Save Models\\Best Models\\DDPG_ip_1_100000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 2000 - Ep: 50\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 2000 - Ep: 50\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 4000 - Ep: 104\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 4000 - Ep: 104\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 5000 - Ep: 131\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 5000 - Ep: 131\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 6000 - Ep: 158\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 6000 - Ep: 158\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 7000 - Ep: 185\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 7000 - Ep: 185\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 8000 - Ep: 212\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 8000 - Ep: 212\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 9000 - Ep: 239\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 9000 - Ep: 239\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 10000 - Ep: 266\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 10000 - Ep: 266\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 11000 - Ep: 293\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 11000 - Ep: 293\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 12000 - Ep: 320\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 12000 - Ep: 320\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 13000 - Ep: 347\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 13000 - Ep: 347\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 14000 - Ep: 374\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 14000 - Ep: 374\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 15000 - Ep: 401\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 15000 - Ep: 401\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 16000 - Ep: 428\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 16000 - Ep: 428\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 17000 - Ep: 455\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 17000 - Ep: 455\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 18000 - Ep: 482\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 18000 - Ep: 482\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 19000 - Ep: 509\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 19000 - Ep: 509\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 20000 - Ep: 536\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 20000 - Ep: 536\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 21000 - Ep: 563\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 21000 - Ep: 563\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 22000 - Ep: 590\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 22000 - Ep: 590\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 23000 - Ep: 617\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 23000 - Ep: 617\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 24000 - Ep: 644\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 24000 - Ep: 644\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 25000 - Ep: 671\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 25000 - Ep: 671\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 26000 - Ep: 698\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 26000 - Ep: 698\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 27000 - Ep: 725\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 27000 - Ep: 725\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 28000 - Ep: 752\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 28000 - Ep: 752\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 29000 - Ep: 779\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 29000 - Ep: 779\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 30000 - Ep: 806\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 30000 - Ep: 806\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 31000 - Ep: 833\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 31000 - Ep: 833\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 32000 - Ep: 860\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 32000 - Ep: 860\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 33000 - Ep: 888\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 33000 - Ep: 888\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 34000 - Ep: 915\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 34000 - Ep: 915\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 35000 - Ep: 942\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 35000 - Ep: 942\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 36000 - Ep: 969\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 36000 - Ep: 969\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 37000 - Ep: 996\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 37000 - Ep: 996\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 38000 - Ep: 1023\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 38000 - Ep: 1023\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 39000 - Ep: 1050\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 39000 - Ep: 1050\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 40000 - Ep: 1077\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 40000 - Ep: 1077\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 41000 - Ep: 1104\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 41000 - Ep: 1104\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 42000 - Ep: 1131\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 42000 - Ep: 1131\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 43000 - Ep: 1158\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 43000 - Ep: 1158\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Num timesteps: 44000 - Ep: 1185\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Num timesteps: 44000 - Ep: 1185\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best reward: 3.97 - Last reward per episode: 3.97 - Mean reward last 10 episode: 3.97 - Delta mean: 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m callback \u001b[38;5;241m=\u001b[39m SaveOnBestTrainingRewardCallback(check_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, log_dir\u001b[38;5;241m=\u001b[39mlog_dir, file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDPG_ip_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(num_cpu, total_timesteps))\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m DDPG(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, \u001b[38;5;241m1000\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#250000 callback=callback\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# vec_env = SubprocVecEnv([make_env(env_id, i, render_mode=\"human\") for i in range(num_cpu)])\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# env = gym.make(env_id, render_mode=\"human\")\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#         obs, rewards, done, info, _ = vec_env.step(action)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# vec_env.render()\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\ddpg\\ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:194\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Delayed policy updates\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_updates \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_delay \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m     actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq1_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    195\u001b[0m     actor_losses\u001b[38;5;241m.\u001b[39mappend(actor_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\policies.py:985\u001b[0m, in \u001b[0;36mContinuousCritic.q1_forward\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    984\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[1;32m--> 985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_networks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def make_env(env_id: str, rank: int, seed: int = 0, render_mode = None, action_type = \"Box\", observation_type = \"Continous\", reward_function = \"Absolute\", task = \"InvertedCartPole\"):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: the environment ID\n",
    "    :param num_env: the number of environments you wish to have in subprocesses\n",
    "    :param seed: the inital seed for RNG\n",
    "    :param rank: index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id, render_mode=render_mode, action_type = action_type, observation_type = observation_type, reward_function = reward_function, task = task) #gym.make(env_id, render_mode=\"human\")\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # log_dir = \"tmpa/\"\n",
    "    # os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    env_id = \"CartPole-v1\"\n",
    "    num_cpu = 1  # Number of processes to use\n",
    "    total_timesteps=100_000 #\n",
    "\n",
    "    log_dir = os.path.join('Training', 'Save Models')\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Create the vectorized environment\n",
    "    # vec_env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n",
    "    # vec_env = VecMonitor(vec_env, log_dir)\n",
    "    env = gym.make(env_id, action_type = \"Box\", observation_type = \"Continous\", reward_function = \"Absolute\", task = \"InvertedCartPole\")\n",
    "    env = Monitor(env, log_dir)\n",
    "\n",
    "    # Stable Baselines provides you with make_vec_env() helper\n",
    "    # which does exactly the previous steps for you.\n",
    "    # You can choose between `DummyVecEnv` (usually faster) and `SubprocVecEnv`\n",
    "    # env = make_vec_env(env_id, n_envs=num_cpu, seed=0, vec_env_cls=SubprocVecEnv)\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir, file_name='DDPG_ip_{}_{}'.format(num_cpu, total_timesteps))\n",
    "\n",
    "\n",
    "    model = DDPG(\"MlpPolicy\", env, verbose=0)\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True, callback=callback) #250000 callback=callback\n",
    "\n",
    "    # vec_env = SubprocVecEnv([make_env(env_id, i, render_mode=\"human\") for i in range(num_cpu)])\n",
    "    # env = gym.make(env_id, render_mode=\"human\")\n",
    "    \n",
    "    # for _ in range(1000):\n",
    "    #     obs= vec_env.reset()\n",
    "    #     done = False\n",
    "    #     while not done:\n",
    "    #         action, _states = model.predict(obs)\n",
    "    #         print(action)\n",
    "    #         obs, rewards, done, info, _ = vec_env.step(action)\n",
    "            # vec_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results([log_dir], total_timesteps, results_plotter.X_TIMESTEPS, \"Inverted-CartPole\")\n",
    "plt.show()\n",
    "plot_results([log_dir], total_timesteps, results_plotter.X_EPISODES, \"Inverted-CartPole\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PPO_Path = os.path.join('Training', 'Save Models', 'PPO_ip_3_500k')\n",
    "model.save(PPO_Path)\n",
    "\n",
    "# Save the policy independently from the model\n",
    "# Note: if you don't save the complete model with `model.save()`\n",
    "# you cannot continue training afterward\n",
    "policy = model.policy\n",
    "PPO_policy_Path = os.path.join('Training', 'Save Policy', 'PPO_ip_3_500k')\n",
    "policy.save(PPO_policy_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded_model has 0 transitions in its buffer\n"
     ]
    }
   ],
   "source": [
    "# the saved model does not contain the replay buffer\n",
    "DDPG_Path = os.path.join('Training', 'Save Models', 'Best Models', 'DDPG_ip_1_50000.zip')\n",
    "loaded_model = DDPG.load(DDPG_Path)\n",
    "print(f\"The loaded_model has {loaded_model.replay_buffer.size()} transitions in its buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=418.60 +/- 2.553283775341697e-05\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Retrieve the environment\n",
    "env = model.get_env()\n",
    "\n",
    "# Load the policy independently from the model\n",
    "PPO_policy_Path = os.path.join('Training', 'Save Policy', 'PPO_ip_3_250k')\n",
    "saved_policy = PPO.load(PPO_policy_Path)\n",
    "\n",
    "# Evaluate the loaded policy\n",
    "mean_reward, std_reward = evaluate_policy(saved_policy, env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode=\"human\", action_type = \"Box\", observation_type = \"Continous\", reward_function = \"Absolute\", task = \"InvertedCartPole\")\n",
    "\n",
    "episodes = 5\n",
    "for episodes in range(1, episodes+1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = loaded_model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action[0])\n",
    "        score += reward\n",
    "    # print('Episode:{} Score:{}'.format(episodes, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs:  [[0.        0.        3.1415927 0.       ]]\n",
      "action:  [[-0.74703765]]\n",
      "obs:  [[ 0.         -0.14576344  3.1415927  -0.21864517]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-2.9152688e-03 -3.4088540e-01  3.1372197e+00 -5.1132810e-01]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.00973298 -0.53594327  3.1269932  -0.80262643]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.02045184 -0.7308436   3.1109407  -1.0906538 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.03506871 -0.9254773   3.0891275  -1.373457  ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.05357826 -1.1197126   3.0616584  -1.6489913 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.07597251 -1.3133909   3.0286787  -1.9151055 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.10224033 -1.5063249   2.9903765  -2.1695373 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.13236682 -1.6983005   2.9469857  -2.4099264 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.16633284 -1.8890837   2.8987873  -2.6338453 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.20411451 -2.0784307   2.8461103  -2.838849  ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.24568312 -2.2661033   2.7893333  -3.0225449 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.2910052 -2.4518876  2.7288826 -3.1826737]]\n",
      "action:  [[-0.8192284]]\n",
      "obs:  [[-0.34004295 -2.6007516   2.665229   -3.2692997 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.39205799 -2.7824523   2.599843   -3.3766932 ]]\n",
      "action:  [[-0.2700026]]\n",
      "obs:  [[-0.44770703 -2.8222888   2.532309   -3.2762947 ]]\n",
      "action:  [[-0.14551035]]\n",
      "obs:  [[-0.50415283 -2.837591    2.4667833  -3.126867  ]]\n",
      "action:  [[0.5438909]]\n",
      "obs:  [[-0.5609046 -2.7218168  2.4042459 -2.807592 ]]\n",
      "action:  [[0.21988612]]\n",
      "obs:  [[-0.61534095 -2.668372    2.348094   -2.5505836 ]]\n",
      "action:  [[0.22871035]]\n",
      "obs:  [[-0.6687084 -2.6140704  2.2970824 -2.2838902]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.7209898 -2.4161193  2.2514045 -1.866894 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.7693122 -2.2200084  2.2140667 -1.4532913]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.81371236 -2.025468    2.185001   -1.0430182 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.8542217  -1.8321704   2.1641405  -0.63565326]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.89086515 -1.6397512   2.1514275  -0.23052205]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.92366016 -1.4478257   2.146817    0.17321783]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.9526167  -1.2559997   2.1502814   0.57650584]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.97773665 -1.0638776   2.1618114   0.98031586]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.9990142  -0.87106925  2.1814177   1.3855968 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-1.0164356  -0.67719764  2.2091298   1.7932107 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-1.0299796  -0.48190948  2.244994    2.2038653 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-1.0396178  -0.28488955  2.2890713   2.618031  ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-1.0453155  -0.08588178  2.3414319   3.0358427 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-1.0470332   0.11528201  2.4021487   3.4569716 ]]\n",
      "action:  [[0.9842164]]\n",
      "obs:  [[-1.0447276   0.31566337  2.4712882   3.8771672 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-1.0384142   0.52107716  2.5488315   4.3012605 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-1.0279927  0.7281979  2.6348567  4.723184 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-1.0134288   0.93638027  2.7293203   5.1389008 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.99470115  1.1446441   2.8320985   5.5429254 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.97180825  1.3516511   2.942957    5.9282284 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.9447752  1.5557318  3.0615215  6.286346 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.9136606  1.7549877 -3.095937   6.6077876]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.87856084  1.9474822  -2.963781    6.8828106 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.83961123  2.1315114  -2.826125    7.1025004 ]]\n",
      "action:  [[0.7761798]]\n",
      "obs:  [[-0.796981   2.2625387 -2.6840749  7.198126 ]]\n",
      "action:  [[0.793576]]\n",
      "obs:  [[-0.7517302  2.3875854 -2.5401125  7.236538 ]]\n",
      "action:  [[0.40165377]]\n",
      "obs:  [[-0.7039785  2.4293766 -2.3953817  7.1218596]]\n",
      "action:  [[-0.3319534]]\n",
      "obs:  [[-0.655391   2.3273058 -2.2529445  6.809853 ]]\n",
      "action:  [[0.33197537]]\n",
      "obs:  [[-0.6088449  2.3489885 -2.1167474  6.60215  ]]\n",
      "action:  [[-0.8641516]]\n",
      "obs:  [[-0.5618651  2.148387  -1.9847044  6.1946497]]\n",
      "action:  [[-0.71341306]]\n",
      "obs:  [[-0.51889735  1.9799577  -1.8608115   5.823865  ]]\n",
      "action:  [[-0.94719017]]\n",
      "obs:  [[-0.4792982  1.7733811 -1.7443341  5.4535317]]\n",
      "action:  [[-0.10543597]]\n",
      "obs:  [[-0.44383058  1.725209   -1.6352634   5.151471  ]]\n",
      "action:  [[0.40762728]]\n",
      "obs:  [[-0.4093264  1.7744029 -1.5322341  4.8628354]]\n",
      "action:  [[-0.737056]]\n",
      "obs:  [[-0.37383834  1.6194104  -1.4349774   4.578017  ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.34145012  1.4202585  -1.343417    4.3271728 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.31304497  1.2241117  -1.2568736   4.1070647 ]]\n",
      "action:  [[-0.7283947]]\n",
      "obs:  [[-0.28856272  1.0800797  -1.1747322   3.8941467 ]]\n",
      "action:  [[0.08605844]]\n",
      "obs:  [[-0.26696113  1.0878433  -1.0968493   3.6184134 ]]\n",
      "action:  [[-0.4374367]]\n",
      "obs:  [[-0.24520427  1.0019253  -1.024481    3.4156396 ]]\n",
      "action:  [[-0.508086]]\n",
      "obs:  [[-0.22516575  0.9046255  -0.95616823  3.2402601 ]]\n",
      "action:  [[-0.38029104]]\n",
      "obs:  [[-0.20707324  0.83234066 -0.8913631   3.0625906 ]]\n",
      "action:  [[0.43337196]]\n",
      "obs:  [[-0.19042644  0.9132117  -0.83011127  2.7576563 ]]\n",
      "action:  [[0.843439]]\n",
      "obs:  [[-0.1721622   1.0730807  -0.77495813  2.3788645 ]]\n",
      "action:  [[-0.5594763]]\n",
      "obs:  [[-0.15070058  0.9708817  -0.7273808   2.282681  ]]\n",
      "action:  [[-0.39724633]]\n",
      "obs:  [[-0.13128296  0.8994242  -0.68172723  2.1672552 ]]\n",
      "action:  [[-0.7414445]]\n",
      "obs:  [[-0.11329447  0.76284975 -0.6383821   2.1410675 ]]\n",
      "action:  [[0.49238357]]\n",
      "obs:  [[-0.09803747  0.8605831  -0.5955607   1.8481451 ]]\n",
      "action:  [[0.48228675]]\n",
      "obs:  [[-0.08082581  0.9572527  -0.55859786  1.5631795 ]]\n",
      "action:  [[0.24841197]]\n",
      "obs:  [[-0.06168076  1.0098249  -0.5273343   1.3404884 ]]\n",
      "action:  [[-0.17901246]]\n",
      "obs:  [[-0.04148426  0.9807881  -0.5005245   1.2301766 ]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.0218685   0.79414076 -0.47592098  1.3347174 ]]\n",
      "action:  [[-0.5470747]]\n",
      "obs:  [[-0.00598568  0.69397706 -0.44922662  1.3335681 ]]\n",
      "action:  [[0.4472258]]\n",
      "obs:  [[ 0.00789386  0.78484344 -0.42255524  1.0831167 ]]\n",
      "action:  [[0.13803892]]\n",
      "obs:  [[ 0.02359073  0.8162856  -0.4008929   0.91953456]]\n",
      "action:  [[-0.30692804]]\n",
      "obs:  [[ 0.03991644  0.7618349  -0.38250223  0.88000405]]\n",
      "action:  [[-0.22068802]]\n",
      "obs:  [[ 0.05515314  0.7238451  -0.36490214  0.8231372 ]]\n",
      "action:  [[-0.11653328]]\n",
      "obs:  [[ 0.06963004  0.70581985 -0.3484394   0.74347866]]\n",
      "action:  [[-0.6100731]]\n",
      "obs:  [[ 0.08374643  0.59216887 -0.33356982  0.80332977]]\n",
      "action:  [[-0.17701405]]\n",
      "obs:  [[ 0.09558982  0.5620961  -0.31750324  0.7496915 ]]\n",
      "action:  [[-0.15073793]]\n",
      "obs:  [[ 0.10683174  0.5369456  -0.3025094   0.6937462 ]]\n",
      "action:  [[0.14328341]]\n",
      "obs:  [[ 0.11757065  0.5686362  -0.28863448  0.56078136]]\n",
      "action:  [[-0.6241676]]\n",
      "obs:  [[ 0.12894337  0.4513686  -0.27741885  0.64572114]]\n",
      "action:  [[-0.03813276]]\n",
      "obs:  [[ 0.13797075  0.44761485 -0.26450443  0.5706175 ]]\n",
      "action:  [[-0.6631052]]\n",
      "obs:  [[ 0.14692304  0.3223906  -0.25309208  0.67506063]]\n",
      "action:  [[0.27647015]]\n",
      "obs:  [[ 0.15337086  0.3794396  -0.23959087  0.51859605]]\n",
      "action:  [[-0.5490646]]\n",
      "obs:  [[ 0.16095965  0.2759752  -0.22921894  0.59959173]]\n",
      "action:  [[-0.24485992]]\n",
      "obs:  [[ 0.16647916  0.23145951 -0.21722712  0.59781694]]\n",
      "action:  [[-0.16269466]]\n",
      "obs:  [[ 0.17110834  0.2027549  -0.20527077  0.5764983 ]]\n",
      "action:  [[0.05447106]]\n",
      "obs:  [[ 0.17516343  0.2161385  -0.1937408   0.4969177 ]]\n",
      "action:  [[-0.6535603]]\n",
      "obs:  [[ 0.1794862   0.09161542 -0.18380246  0.6236036 ]]\n",
      "action:  [[0.7029286]]\n",
      "obs:  [[ 0.18131852  0.23093937 -0.17133038  0.3644037 ]]\n",
      "action:  [[-0.07497853]]\n",
      "obs:  [[ 0.1859373   0.21872263 -0.16404231  0.33233544]]\n",
      "action:  [[-0.26022264]]\n",
      "obs:  [[ 0.19031176  0.17033494 -0.1573956   0.35593015]]\n",
      "action:  [[-0.18174535]]\n",
      "obs:  [[ 0.19371845  0.13713294 -0.15027699  0.35903406]]\n",
      "action:  [[-0.2847505]]\n",
      "obs:  [[ 0.19646111  0.08376345 -0.14309631  0.3941707 ]]\n",
      "action:  [[0.17563653]]\n",
      "obs:  [[ 0.19813639  0.11998279 -0.1352129   0.2984701 ]]\n",
      "action:  [[0.08178623]]\n",
      "obs:  [[ 0.20053604  0.13782124 -0.1292435   0.23232508]]\n",
      "action:  [[-0.5254322]]\n",
      "obs:  [[ 0.20329246  0.03724631 -0.124597    0.34403735]]\n",
      "action:  [[-0.397633]]\n",
      "obs:  [[ 0.20403738 -0.03850089 -0.11771625  0.42024055]]\n",
      "action:  [[0.7993625]]\n",
      "obs:  [[ 0.20326737  0.11896578 -0.10931144  0.15114646]]\n",
      "action:  [[0.50855935]]\n",
      "obs:  [[ 0.2056467   0.21966198 -0.10628851 -0.03106992]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[ 0.21003993  0.02621212 -0.10690991  0.2262773 ]]\n",
      "action:  [[-0.63931316]]\n",
      "obs:  [[ 0.21056417 -0.09691308 -0.10238436  0.37853897]]\n",
      "action:  [[-0.6852062]]\n",
      "obs:  [[ 0.20862591 -0.22906701 -0.09481359  0.5456834 ]]\n",
      "action:  [[1.]]\n",
      "obs:  [[ 0.20404457 -0.03274969 -0.08389992  0.22469653]]\n",
      "action:  [[-0.98843426]]\n",
      "obs:  [[ 0.20338957 -0.22432299 -0.07940599  0.48640805]]\n",
      "action:  [[0.73270345]]\n",
      "obs:  [[ 0.19890311 -0.08030705 -0.06967782  0.24774401]]\n",
      "action:  [[0.38495338]]\n",
      "obs:  [[ 0.19729698 -0.0042293  -0.06472294  0.11343557]]\n",
      "action:  [[-0.3344836]]\n",
      "obs:  [[ 0.19721238 -0.06854989 -0.06245423  0.19069918]]\n",
      "action:  [[-0.0063118]]\n",
      "obs:  [[ 0.19584139 -0.06889021 -0.05864025  0.17285906]]\n",
      "action:  [[-0.32471663]]\n",
      "obs:  [[ 0.19446358 -0.1313965  -0.05518307  0.24922697]]\n",
      "action:  [[0.10204808]]\n",
      "obs:  [[ 0.19183566 -0.11070282 -0.05019853  0.20201813]]\n",
      "action:  [[0.17783296]]\n",
      "obs:  [[ 0.1896216  -0.07529352 -0.04615816  0.1342189 ]]\n",
      "action:  [[0.0022727]]\n",
      "obs:  [[ 0.18811573 -0.07419001 -0.04347379  0.11899973]]\n",
      "action:  [[-0.05600914]]\n",
      "obs:  [[ 0.18663193 -0.0844951  -0.04109379  0.12166549]]\n",
      "action:  [[-0.3415187]]\n",
      "obs:  [[ 0.18494202 -0.15053666 -0.03866048  0.20856601]]\n",
      "action:  [[0.3291926]]\n",
      "obs:  [[ 0.18193129 -0.08575878 -0.03448916  0.10010845]]\n",
      "action:  [[0.1410488]]\n",
      "obs:  [[ 0.18021612 -0.0577456  -0.03248699  0.04797587]]\n",
      "action:  [[-0.38746482]]\n",
      "obs:  [[ 0.1790612  -0.13287719 -0.03152747  0.15106428]]\n",
      "action:  [[-0.03441282]]\n",
      "obs:  [[ 0.17640366 -0.13914028 -0.02850619  0.1511867 ]]\n",
      "action:  [[0.76863897]]\n",
      "obs:  [[ 0.17362085  0.01123708 -0.02548246 -0.08266737]]\n",
      "action:  [[0.30498347]]\n",
      "obs:  [[ 0.1738456   0.07110833 -0.0271358  -0.17993613]]\n",
      "action:  [[-0.4609447]]\n",
      "obs:  [[ 0.17526777 -0.01843915 -0.03073453 -0.05364129]]\n",
      "action:  [[0.28731436]]\n",
      "obs:  [[ 0.17489898  0.03805869 -0.03180735 -0.14738257]]\n",
      "action:  [[-0.49125534]]\n",
      "obs:  [[ 0.17566015 -0.05733376 -0.034755   -0.01371604]]\n",
      "action:  [[-0.6080593]]\n",
      "obs:  [[ 0.17451347 -0.17547101 -0.03502933  0.1531669 ]]\n",
      "action:  [[0.52971256]]\n",
      "obs:  [[ 0.17100406 -0.07162063 -0.03196599 -0.01280963]]\n",
      "action:  [[-0.27563912]]\n",
      "obs:  [[ 0.16957165 -0.12494176 -0.03222218  0.05773481]]\n",
      "action:  [[-0.20036791]]\n",
      "obs:  [[ 0.16707282 -0.16357331 -0.03106748  0.10618038]]\n",
      "action:  [[-0.1514333]]\n",
      "obs:  [[ 0.16380134 -0.1926743  -0.02894388  0.14067842]]\n",
      "action:  [[-0.14607643]]\n",
      "obs:  [[ 0.15994786 -0.22076099 -0.02613031  0.1742825 ]]\n",
      "action:  [[-0.57065547]]\n",
      "obs:  [[ 0.15553264 -0.33172905 -0.02264466  0.33299634]]\n",
      "action:  [[0.88206327]]\n",
      "obs:  [[ 0.14889807 -0.15930341 -0.01598473  0.06776723]]\n",
      "action:  [[-0.08006667]]\n",
      "obs:  [[ 0.14571199 -0.17469676 -0.01462939  0.08615499]]\n",
      "action:  [[0.19623363]]\n",
      "obs:  [[ 0.14221805 -0.13619821 -0.01290629  0.02411244]]\n",
      "action:  [[-0.02292241]]\n",
      "obs:  [[ 0.13949409 -0.14048575 -0.01242404  0.02674889]]\n",
      "action:  [[0.46349213]]\n",
      "obs:  [[ 0.13668437 -0.04987113 -0.01188906 -0.11281512]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[ 0.13568695 -0.24482073 -0.01414536  0.1760933 ]]\n",
      "action:  [[0.23822819]]\n",
      "obs:  [[ 0.13079055 -0.19813545 -0.0106235   0.10191379]]\n",
      "action:  [[0.51837486]]\n",
      "obs:  [[ 0.12682784 -0.09683773 -0.00858522 -0.05314745]]\n",
      "action:  [[0.08535722]]\n",
      "obs:  [[ 0.12489107 -0.08005966 -0.00964817 -0.08083765]]\n",
      "action:  [[-0.78680384]]\n",
      "obs:  [[ 0.12328988 -0.23344302 -0.01126492  0.14639015]]\n",
      "action:  [[-0.2615196]]\n",
      "obs:  [[ 0.11862102 -0.28430945 -0.00833712  0.21937315]]\n",
      "action:  [[0.83663094]]\n",
      "obs:  [[ 0.11293484 -0.12094606 -0.00394966 -0.02811451]]\n",
      "action:  [[-0.0124993]]\n",
      "obs:  [[ 0.11051591 -0.1233283  -0.00451195 -0.02570237]]\n",
      "action:  [[-0.2780368]]\n",
      "obs:  [[ 0.10804935 -0.1775146  -0.00502599  0.05424974]]\n",
      "action:  [[0.13326444]]\n",
      "obs:  [[ 0.10449906 -0.15143977 -0.003941    0.01366035]]\n",
      "action:  [[-0.09519136]]\n",
      "obs:  [[ 0.10147026 -0.16995715 -0.00366779  0.04027756]]\n",
      "action:  [[-0.59180224]]\n",
      "obs:  [[ 0.09807111 -0.28537804 -0.00286224  0.21232942]]\n",
      "action:  [[1.]]\n",
      "obs:  [[ 0.09236355 -0.0902153   0.00138435 -0.08125502]]\n",
      "action:  [[-0.01785696]]\n",
      "obs:  [[ 0.09055925 -0.09371942 -0.00024075 -0.07559183]]\n",
      "action:  [[-0.26385707]]\n",
      "obs:  [[ 0.08868486 -0.14520028 -0.00175259  0.00155867]]\n",
      "action:  [[-0.7099549]]\n",
      "obs:  [[ 0.08578085 -0.2837029  -0.00172141  0.20879702]]\n",
      "action:  [[-0.06894836]]\n",
      "obs:  [[ 0.08010679 -0.29713163  0.00245453  0.22843398]]\n",
      "action:  [[0.26306406]]\n",
      "obs:  [[ 0.07416417 -0.24583715  0.00702321  0.15221412]]\n",
      "action:  [[0.0583719]]\n",
      "obs:  [[ 0.06924742 -0.2345481   0.01006749  0.13734579]]\n",
      "action:  [[0.6863225]]\n",
      "obs:  [[ 0.06455646 -0.1007767   0.0128144  -0.06034135]]\n",
      "action:  [[0.3008729]]\n",
      "obs:  [[ 0.06254093 -0.04225421  0.01160758 -0.14435056]]\n",
      "action:  [[-0.7078644]]\n",
      "obs:  [[ 0.06169584 -0.18053895  0.00872057  0.06647513]]\n",
      "action:  [[-0.19338657]]\n",
      "obs:  [[ 0.05808506 -0.21839772  0.01005007  0.12582496]]\n",
      "action:  [[0.995278]]\n",
      "obs:  [[ 0.05371711 -0.02434254  0.01256657 -0.16228846]]\n",
      "action:  [[-0.41883284]]\n",
      "obs:  [[ 0.05323026 -0.10624496  0.0093208  -0.03575006]]\n",
      "action:  [[-0.46631834]]\n",
      "obs:  [[ 0.05110536 -0.19736698  0.0086058   0.10366731]]\n",
      "action:  [[0.6630921]]\n",
      "obs:  [[ 0.04715802 -0.06810718  0.01067914 -0.08768513]]\n",
      "action:  [[-0.6748555]]\n",
      "obs:  [[ 0.04579587 -0.19993825  0.00892544  0.11318982]]\n",
      "action:  [[0.5895434]]\n",
      "obs:  [[ 0.04179711 -0.08503395  0.01118924 -0.05653572]]\n",
      "action:  [[-0.17435864]]\n",
      "obs:  [[ 0.04009643 -0.11921526  0.01005852 -0.00197741]]\n",
      "action:  [[0.41957217]]\n",
      "obs:  [[ 0.03771212 -0.03749237  0.01001897 -0.12159839]]\n",
      "action:  [[-0.31470475]]\n",
      "obs:  [[ 0.03696228 -0.09904125  0.00758701 -0.02633417]]\n",
      "action:  [[0.14214829]]\n",
      "obs:  [[ 0.03498145 -0.07141392  0.00706032 -0.06554342]]\n",
      "action:  [[-0.09607986]]\n",
      "obs:  [[ 0.03355317 -0.09026236  0.00574945 -0.03519574]]\n",
      "action:  [[-0.88666177]]\n",
      "obs:  [[ 0.03174793 -0.26335156  0.00504554  0.2261241 ]]\n",
      "action:  [[0.12987098]]\n",
      "obs:  [[ 0.02648089 -0.23808303  0.00956802  0.18970518]]\n",
      "action:  [[0.06709266]]\n",
      "obs:  [[ 0.02171923 -0.22512875  0.01336213  0.17308758]]\n",
      "action:  [[0.5960522]]\n",
      "obs:  [[ 0.01721666 -0.10901862  0.01682388  0.00286628]]\n",
      "action:  [[0.00976677]]\n",
      "obs:  [[ 0.01503629 -0.10735417  0.0168812   0.00531595]]\n",
      "action:  [[0.7009752]]\n",
      "obs:  [[ 0.0128892   0.02917657  0.01698752 -0.19448814]]\n",
      "action:  [[-0.6241368]]\n",
      "obs:  [[ 0.01347273 -0.09284659  0.01309776 -0.00648571]]\n",
      "action:  [[-0.14506476]]\n",
      "obs:  [[ 0.0116158  -0.12133937  0.01296804  0.04010042]]\n",
      "action:  [[0.01623116]]\n",
      "obs:  [[ 0.00918902 -0.11835829  0.01377005  0.03944167]]\n",
      "action:  [[0.8287212]]\n",
      "obs:  [[ 0.00682185  0.04314373  0.01455889 -0.19874012]]\n",
      "action:  [[-0.37139297]]\n",
      "obs:  [[ 0.00768472 -0.02953027  0.01058408 -0.08546052]]\n",
      "action:  [[-0.00245856]]\n",
      "obs:  [[ 0.00709412 -0.03016168  0.00887487 -0.08140179]]\n",
      "action:  [[-0.27495992]]\n",
      "obs:  [[ 0.00649089 -0.08393931  0.00724684  0.00187065]]\n",
      "action:  [[0.37175408]]\n",
      "obs:  [[ 0.0048121  -0.01150613  0.00728425 -0.10464571]]\n",
      "action:  [[-0.12364966]]\n",
      "obs:  [[ 0.00458198 -0.03573718  0.00519134 -0.06615855]]\n",
      "action:  [[-0.26487833]]\n",
      "obs:  [[ 0.00386723 -0.08749508  0.00386817  0.01300351]]\n",
      "action:  [[-0.1971585]]\n",
      "obs:  [[ 0.00211733 -0.12602046  0.00412824  0.07192839]]\n",
      "action:  [[0.87779677]]\n",
      "obs:  [[-0.00040308  0.04519755  0.0055668  -0.18368275]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[ 0.00050087 -0.15000361  0.00189315  0.11075108]]\n",
      "action:  [[0.03282984]]\n",
      "obs:  [[-0.0024992  -0.14362492  0.00410817  0.10173965]]\n",
      "action:  [[0.19147962]]\n",
      "obs:  [[-0.0053717  -0.10632196  0.00614296  0.04699348]]\n",
      "action:  [[0.25141728]]\n",
      "obs:  [[-0.00749814 -0.05735315  0.00708283 -0.02465233]]\n",
      "action:  [[-0.4105476]]\n",
      "obs:  [[-0.0086452  -0.13756128  0.00658979  0.09773917]]\n",
      "action:  [[0.04479572]]\n",
      "obs:  [[-0.01139642 -0.12891512  0.00854457  0.08670761]]\n",
      "action:  [[0.25787035]]\n",
      "obs:  [[-0.01397473 -0.07872169  0.01027872  0.01393229]]\n",
      "action:  [[-0.7383108]]\n",
      "obs:  [[-0.01554916 -0.22292863  0.01055737  0.23325315]]\n",
      "action:  [[1.]]\n",
      "obs:  [[-0.02000773 -0.0279591   0.01522243 -0.05608103]]\n",
      "action:  [[-0.50203365]]\n",
      "obs:  [[-0.02056691 -0.12613346  0.01410081  0.09563866]]\n",
      "action:  [[0.2446074]]\n",
      "obs:  [[-0.02308958 -0.07860794  0.01601358  0.02850298]]\n",
      "action:  [[0.18011928]]\n",
      "obs:  [[-0.02466174 -0.04369298  0.01658364 -0.01915495]]\n",
      "action:  [[-0.33980706]]\n",
      "obs:  [[-0.0255356  -0.11023324  0.01620054  0.08551708]]\n",
      "action:  [[-0.08766726]]\n",
      "obs:  [[-0.02774027 -0.1275709   0.01791088  0.1162829 ]]\n",
      "action:  [[-0.01461005]]\n",
      "obs:  [[-0.03029169 -0.13067815  0.02023654  0.12620854]]\n",
      "action:  [[0.47528508]]\n",
      "obs:  [[-0.03290525 -0.03823219  0.02276071 -0.00648286]]\n",
      "action:  [[0.09956263]]\n",
      "obs:  [[-0.03366989 -0.01913237  0.02263106 -0.0284341 ]]\n",
      "action:  [[0.3565716]]\n",
      "obs:  [[-0.03405254  0.05011556  0.02206237 -0.12562642]]\n",
      "action:  [[0.2359645]]\n",
      "obs:  [[-0.03305023  0.09583982  0.01954985 -0.18771031]]\n",
      "action:  [[-0.47532982]]\n",
      "obs:  [[-0.03113343  0.00281551  0.01579564 -0.04245322]]\n",
      "action:  [[0.15612146]]\n",
      "obs:  [[-0.03107712  0.03305121  0.01494658 -0.08315739]]\n",
      "action:  [[0.2134295]]\n",
      "obs:  [[-0.0304161   0.07448109  0.01328343 -0.14090115]]\n",
      "action:  [[0.07594098]]\n",
      "obs:  [[-0.02892647  0.08910843  0.0104654  -0.158935  ]]\n",
      "action:  [[-0.34491605]]\n",
      "obs:  [[-0.02714431  0.02165846  0.00728671 -0.05468881]]\n",
      "action:  [[-0.10281923]]\n",
      "obs:  [[-0.02671114  0.00149177  0.00619293 -0.02229731]]\n",
      "action:  [[0.24863678]]\n",
      "obs:  [[-0.0266813   0.04991732  0.00574698 -0.09311353]]\n",
      "action:  [[-0.44136238]]\n",
      "obs:  [[-0.02568296 -0.03628433  0.00388471  0.03787641]]\n",
      "action:  [[0.39475107]]\n",
      "obs:  [[-0.02640864  0.04068448  0.00464224 -0.07643383]]\n",
      "action:  [[0.6182213]]\n",
      "obs:  [[-0.02559495  0.16124628  0.00311356 -0.25590977]]\n",
      "action:  [[-1.]]\n",
      "obs:  [[-0.02237003 -0.03391999 -0.00200463  0.0377536 ]]\n",
      "action:  [[0.23738891]]\n",
      "obs:  [[-0.02304843  0.01242854 -0.00124956 -0.0323584 ]]\n",
      "action:  [[0.09758919]]\n",
      "obs:  [[-0.02279986  0.03148825 -0.00189673 -0.06131532]]\n",
      "action:  [[-0.11459039]]\n",
      "obs:  [[-0.02217009  0.00915635 -0.00312303 -0.02837517]]\n",
      "action:  [[0.24359937]]\n",
      "obs:  [[-0.02198697  0.05673268 -0.00369054 -0.10065749]]\n",
      "action:  [[-0.21541417]]\n",
      "obs:  [[-0.02085231  0.01475358 -0.00570369 -0.03877429]]\n",
      "action:  [[0.0978542]]\n",
      "obs:  [[-0.02055724  0.03392883 -0.00647917 -0.06921356]]\n",
      "action:  [[0.04093072]]\n",
      "obs:  [[-0.01987866  0.04200817 -0.00786345 -0.08323719]]\n",
      "action:  [[-0.4219045]]\n",
      "obs:  [[-0.0190385  -0.04020157 -0.00952819  0.03776178]]\n",
      "action:  [[0.1078165]]\n",
      "obs:  [[-0.01984253 -0.01902772 -0.00877295  0.0032012 ]]\n",
      "action:  [[0.5035038]]\n",
      "obs:  [[-0.02022308  0.07934219 -0.00870893 -0.1469272 ]]\n",
      "action:  [[0.07092816]]\n",
      "obs:  [[-0.01863624  0.09330645 -0.01164747 -0.1704332 ]]\n",
      "action:  [[-0.3832351]]\n",
      "obs:  [[-0.01677011  0.01869631 -0.01505614 -0.06194985]]\n",
      "action:  [[-0.51703346]]\n",
      "obs:  [[-0.01639619 -0.08197076 -0.01629513  0.0846073 ]]\n",
      "action:  [[0.43376905]]\n",
      "obs:  [[-0.0180356   0.002899   -0.01460299 -0.04747099]]\n",
      "action:  [[0.08614913]]\n",
      "obs:  [[-0.01797762  0.01991768 -0.01555241 -0.07728942]]\n",
      "action:  [[-0.88840675]]\n",
      "obs:  [[-0.01757927 -0.153204   -0.0170982   0.17778946]]\n",
      "action:  [[0.09638332]]\n",
      "obs:  [[-0.02064335 -0.13415326 -0.01354241  0.14419092]]\n",
      "action:  [[0.5116854]]\n",
      "obs:  [[-0.02332641 -0.03411964 -0.01065859 -0.0098271 ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs: \u001b[39m\u001b[38;5;124m\"\u001b[39m , obs)\n\u001b[1;32m----> 7\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction: \u001b[39m\u001b[38;5;124m\"\u001b[39m , action)\n\u001b[0;32m      9\u001b[0m     obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:553\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    535\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\policies.py:366\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    363\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 366\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    368\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\policies.py:715\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    708\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\policies.py:750\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    748\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_features_extractor)\n\u001b[0;32m    749\u001b[0m latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(features)\n\u001b[1;32m--> 750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\policies.py:692\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    689\u001b[0m mean_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_net(latent_pi)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\distributions.py:163\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[1;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproba_distribution\u001b[39m(\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDiagGaussianDistribution, mean_actions: th\u001b[38;5;241m.\u001b[39mTensor, log_std: th\u001b[38;5;241m.\u001b[39mTensor\n\u001b[0;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDiagGaussianDistribution:\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    Create the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     action_std \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlog_std\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m Normal(mean_actions, action_std)\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vec_env = SubprocVecEnv([make_env(env_id, i, render_mode=\"human\") for i in range(1)]) #num_cpu\n",
    "vec_env = VecMonitor(vec_env)\n",
    "obs = vec_env.reset()\n",
    "obs_2_values = []\n",
    "for _ in range(500):\n",
    "    print(\"obs: \" , obs)\n",
    "    action, _states = model.predict(obs)\n",
    "    print(\"action: \" , action)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    # obs_2_values.append(obs[0][2])\n",
    "    vec_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\multiprocessing\\connection.py:312\u001b[0m, in \u001b[0;36mPipeConnection._recv_bytes\u001b[1;34m(self, maxsize)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m     nread, err \u001b[38;5;241m=\u001b[39m \u001b[43mov\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetOverlappedResult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [WinError 109] The pipe has been ended",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     vec_env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[1;32m---> 13\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43mvec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print('Episode:{} Score:{}'.format(episodes, score))\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:76\u001b[0m, in \u001b[0;36mVecMonitor.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 76\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py:129\u001b[0m, in \u001b[0;36mSubprocVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m [remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py:129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\multiprocessing\\connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[1;32md:\\anacoda3\\envs\\RL-env\\lib\\multiprocessing\\connection.py:321\u001b[0m, in \u001b[0;36mPipeConnection._recv_bytes\u001b[1;34m(self, maxsize)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwinerror \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mERROR_BROKEN_PIPE:\n\u001b[1;32m--> 321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vec_env = SubprocVecEnv([make_env(env_id, i, render_mode=\"human\") for i in range(num_cpu)])\n",
    "vec_env = VecMonitor(vec_env)\n",
    "\n",
    "episodes = 30\n",
    "for episodes in range(1, episodes+1):\n",
    "    obs = vec_env.reset()\n",
    "    done = [False, False, False, False]\n",
    "    score = 0\n",
    "\n",
    "    while not done[0]:\n",
    "        vec_env.render()\n",
    "        action = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = vec_env.step(action)\n",
    "        score += reward\n",
    "    # print('Episode:{} Score:{}'.format(episodes, score))\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([334])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_env.episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_env.episode_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3\n",
    "b = 5 \n",
    "log_dir = os.path.join('Training', 'Save Models', 'Best Models', 'PPO_ip_{}_{}k'.format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\Save Models\\Best Models\\PPO_ip_3_5k\n"
     ]
    }
   ],
   "source": [
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.820053], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_env.episode_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v1'\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "episodes = 30\n",
    "for episodes in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = [False, False, False, False]\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, truncated, info = env.step(action)\n",
    "        print(n_state)\n",
    "        score += reward\n",
    "    # print('Episode:{} Score:{}'.format(episodes, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = SubprocVecEnv([make_env(env_id, i, render_mode=\"human\") for i in range(num_cpu)])\n",
    "vec_env = VecMonitor(vec_env)\n",
    "obs = vec_env.reset()\n",
    "obs_2_values = []\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    # obs_2_values.append(obs[0][2])\n",
    "    vec_env.render()\n",
    "    model.train()\n",
    "\n",
    "episodes = 30\n",
    "for episodes in range(1, episodes+1):\n",
    "    state = vec_env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, truncated, info = env.step(action)\n",
    "        print(n_state)\n",
    "        score += reward\n",
    "    # print('Episode:{} Score:{}'.format(episodes, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = SubprocVecEnv([make_env(env_id, i, render_mode=\"human\") for i in range(num_cpu)])\n",
    "vec_env = VecMonitor(vec_env)\n",
    "obs = vec_env.reset()\n",
    "obs_2_values = []\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    # obs_2_values.append(obs[0][2])\n",
    "    vec_env.render()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,  34, 100, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_env.episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_episode_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env(env_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2_array = np.array(obs_2_values).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2_array[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(obs_2_array[2])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('obs[2]')\n",
    "plt.title('Value of obs[2] over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(env_id, render_mode=\"human\")\n",
    "# # status = env.observation_space.shape[0]\n",
    "# actions = env.action_space.n\n",
    "\n",
    "# episodes = 1000\n",
    "# for episode in range(1, episodes+1):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     score = 0\n",
    "\n",
    "\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action, _states = model.predict(obs)\n",
    "#         obs, reward, done, truncated, info = env.step(action)\n",
    "#         score += reward\n",
    "#     print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose >= 1:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose >= 1:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "# env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "# env = Monitor(env, log_dir)\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "num_cpu = 4  # Number of processes to use\n",
    "# Create and wrap the environment\n",
    "env = gym.make(env_id)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "\n",
    "# Add some action noise for exploration\n",
    "# n_actions = env.action_space.shape[-1]\n",
    "# action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "# Because we use parameter noise, we should use a MlpPolicy with layer normalization\n",
    "# model = TD3(\"MlpPolicy\", env, action_noise=action_noise, verbose=0)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "# Train the agent\n",
    "timesteps = 1e5\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"TD3 LunarLander\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\n",
    "status = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    status = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1])\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
